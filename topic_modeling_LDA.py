# -*- coding: utf-8 -*-
"""topic modeling-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dRGv0RRQ4Yob6gUR-0A17PsBzPPioAvk
"""

from google.colab import files
uploaded = files.upload()

# Install required packages

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import warnings
warnings.filterwarnings('ignore')

# NLP libraries
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import re

# Topic modeling
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import pyLDAvis
import pyLDAvis.lda_model

# Download NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')

# Load the dataset
df = pd.read_excel('preprocessed_data.xlsx')
print(f"Dataset shape: {df.shape}")
print(f"Columns: {df.columns.tolist()}")

# Install required packages for systematic text correction
!pip install pyspellchecker textblob autocorrect

import pandas as pd
import re
from collections import Counter
import string

# Autocorrect libraries
from spellchecker import SpellChecker
from textblob import TextBlob
from autocorrect import Speller

class SystematicTextCorrector:
    def __init__(self):
        # Initialize spell checkers
        self.spell_checker = SpellChecker()
        self.textblob_corrector = TextBlob
        self.autocorrect_spell = Speller(lang='en')

        # Add mining-specific vocabulary
        mining_vocabulary = {
            'workman', 'workmen', 'mazdoor', 'sirdar', 'overman', 'fitter',
            'driller', 'blaster', 'loader', 'dumper', 'tipper', 'dozer',
            'excavator', 'shovel', 'conveyor', 'overburden', 'haul', 'haulage',
            'opencast', 'underground', 'longwall', 'roadway', 'gallery',
            'incline', 'decline', 'shaft', 'cage', 'skip', 'winder',
            'methane', 'coalface', 'goaf', 'stook', 'pack', 'chock',
            'hydraulic', 'pneumatic', 'compressor', 'ventilation', 'airway'
        }

        # Add mining vocabulary to spell checker
        self.spell_checker.word_frequency.load_words(mining_vocabulary)

    def detect_concatenated_words(self, text):
        """
        Detect words that are likely concatenated (like 'aworker', 'theoperator')
        """
        words = text.split()
        corrected_words = []

        for word in words:
            if len(word) > 6:  # Only check longer words
                # Check if word starts with common articles/prepositions
                prefixes = ['a', 'an', 'the', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']

                for prefix in prefixes:
                    if word.lower().startswith(prefix) and len(word) > len(prefix) + 2:
                        potential_split = prefix + ' ' + word[len(prefix):]

                        # Check if the split makes more sense
                        remaining_word = word[len(prefix):]
                        if remaining_word.lower() in self.spell_checker or len(remaining_word) > 4:
                            corrected_words.append(potential_split)
                            break
                else:
                    corrected_words.append(word)
            else:
                corrected_words.append(word)

        return ' '.join(corrected_words)

    def detect_split_words(self, text):
        """
        Detect words that are incorrectly split (like 'work er', 'oper ator')
        """
        words = text.split()
        corrected_words = []
        i = 0

        while i < len(words):
            current_word = words[i]

            # If current word is very short and next word exists
            if i + 1 < len(words) and len(current_word) <= 3:
                next_word = words[i + 1]
                combined = current_word + next_word

                # Check if combined word is more likely to be correct
                if (combined.lower() in self.spell_checker or
                    self.spell_checker.correction(combined) == combined.lower()):
                    corrected_words.append(combined)
                    i += 2  # Skip next word as it's been combined
                    continue

            corrected_words.append(current_word)
            i += 1

        return ' '.join(corrected_words)

    def systematic_spell_check(self, text):
        """
        Apply systematic spell checking using multiple methods
        """
        words = text.split()
        corrected_words = []

        for word in words:
            # Skip very short words and numbers
            if len(word) <= 2 or word.isdigit():
                corrected_words.append(word)
                continue

            # Clean word for checking
            clean_word = word.lower().strip(string.punctuation)

            # Check if word is correct
            if clean_word in self.spell_checker:
                corrected_words.append(word)
            else:
                # Get correction suggestions
                suggestions = self.spell_checker.candidates(clean_word)

                if suggestions:
                    # Use the most likely correction
                    best_correction = min(suggestions, key=lambda x: self.spell_checker.word_frequency[x])

                    # Only apply correction if it's significantly better
                    if len(best_correction) >= len(clean_word) - 1:  # Allow for minor differences
                        corrected_word = word.replace(clean_word, best_correction)
                        corrected_words.append(corrected_word)
                    else:
                        corrected_words.append(word)
                else:
                    corrected_words.append(word)

        return ' '.join(corrected_words)

    def correct_text(self, text):
        """
        Apply systematic text correction pipeline
        """
        if pd.isna(text) or text == '':
            return ''

        text = str(text).strip()

        # Step 1: Fix concatenated words (aworker -> a worker)
        text = self.detect_concatenated_words(text)

        # Step 2: Fix split words (work er -> worker)
        text = self.detect_split_words(text)

        # Step 3: Apply spell checking
        text = self.systematic_spell_check(text)

        # Step 4: Clean up extra spaces
        text = re.sub(r'\s+', ' ', text).strip()

        return text

# Initialize the corrector
corrector = SystematicTextCorrector()

# Test on your problematic samples first
test_samples = [
    "While agroup of five workmen unloading TOR Steel Rod bundles by 10T mobile crane from alorry at area store on surface, suddenly one bundle fell down on legs of aworkman, causing serious injuries",
    "While acontractor'sworkman unauthorizedly drove atractor inside an opencast workings, which is prohibited by order, it toppled hitting aboulder on the berm of ahaul road to which he received head inju",
    "While adozer was deployed for making ramp with its blade in raise dposition in close proximity of another worker, hit amazdoor present in the working area of dozer thereby causing to serious bodily in",
    "While atimber Mistry, authorized to work as conveyor operator, was trying to remove acoal piece stuck near discharge drum of arunning belt conveyor with ahand shovel in abelowground coal mine, he was"
]

print("=== Testing Systematic Autocorrect ===")
for i, sample in enumerate(test_samples, 1):
    print(f"\nSample {i} Original:")
    print(sample)

    corrected = corrector.correct_text(sample)
    print(f"\nSample {i} Corrected:")
    print(corrected)
    print("-" * 80)

def comprehensive_quality_check(df, column_name):
    """
    Comprehensive quality assessment of corrected text
    """
    print(f"\n=== Comprehensive Quality Check for {column_name} ===")

    sample_texts = df[column_name].dropna().sample(5).tolist()

    for i, text in enumerate(sample_texts, 1):
        print(f"\nSample {i}:")
        print(text[:400] + "..." if len(text) > 400 else text)

    # Statistical analysis
    all_text = ' '.join(df[column_name].dropna().astype(str))
    all_words = all_text.lower().split()

    # Check for remaining problematic patterns
    single_letters = [word for word in all_words if len(word) == 1 and word.isalpha()]
    single_letter_count = Counter(single_letters)

    # Filter out legitimate single letters
    legitimate_singles = {'a', 'i'}
    problematic_singles = {k: v for k, v in single_letter_count.items() if k not in legitimate_singles}

    if problematic_singles:
        print(f"\nRemaining single-letter issues: {problematic_singles}")
    else:
        print(f"\nNo problematic single letters found!")

    # Check for very short words that might be fragments
    short_words = [word for word in all_words if 2 <= len(word) <= 3]
    short_word_freq = Counter(short_words).most_common(10)

    print(f"\nMost common 2-3 letter words (potential fragments): {short_word_freq}")

    # Vocabulary richness
    unique_words = len(set(all_words))
    total_words = len(all_words)
    print(f"\nVocabulary statistics:")
    print(f"Total words: {total_words}")
    print(f"Unique words: {unique_words}")
    print(f"Vocabulary richness: {unique_words/total_words:.3f}")

# Run comprehensive quality check
comprehensive_quality_check(df, 'Description')
comprehensive_quality_check(df, 'Suggestions')

# Save the systematically corrected data
df.to_excel('preprocessed_data_systematic_correction.xlsx', index=False)
print("\nSystematically corrected data saved!")

import pandas as pd
import re

def improved_text_correction(text):
    """
    Simple but effective text correction focusing on the actual problems
    """
    if pd.isna(text) or text == '':
        return ''

    text = str(text).strip()

    # Fix the main issue: article concatenations
    # Use word boundaries to be precise

    # Fix "a" + word concatenations
    text = re.sub(r'\ba([a-z]{3,})\b', r'a \1', text, flags=re.IGNORECASE)

    # Fix "an" + word concatenations
    text = re.sub(r'\ban([a-z]{3,})\b', r'an \1', text, flags=re.IGNORECASE)

    # Fix "the" + word concatenations
    text = re.sub(r'\bthe([a-z]{3,})\b', r'the \1', text, flags=re.IGNORECASE)

    # Fix common split words (specific patterns we know are problematic)
    split_word_fixes = {
        r'\bbl asting\b': 'blasting',
        r'\bwork er\b': 'worker',
        r'\bwork ers\b': 'workers',
        r'\bwork ing\b': 'working',
        r'\boper ator\b': 'operator',
        r'\boper ating\b': 'operating',
        r'\bsuper visor\b': 'supervisor',
        r'\bunder ground\b': 'underground',
        r'\bover burden\b': 'overburden',
        r'\bequip ment\b': 'equipment',
        r'\bmach ine\b': 'machine',
        r'\bmach inery\b': 'machinery',
        r'\bveh icle\b': 'vehicle',
        r'\bcon veyor\b': 'conveyor',
        r'\bel evator\b': 'elevator',
        r'\bel ectric\b': 'electric',
        r'\bel ectrician\b': 'electrician',
        r'\bmain tenance\b': 'maintenance',
        r'\bload ing\b': 'loading',
        r'\bunload ing\b': 'unloading'
    }

    # Apply split word fixes
    for pattern, replacement in split_word_fixes.items():
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)

    # Clean up multiple spaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Test the improved correction on the problematic samples
test_samples = [
    "While agroup of five workmen unloading TOR Steel Rod bundles by 10T mobile crane from alorry",
    "While acontractor'sworkman unauthorizedly drove atractor inside an opencast workings",
    "While adozer was deployed for making ramp with its blade in raise dposition",
    "While acrew of seven support personnel were erecting steel chock at goaf edge of asplit gallery in depillaring panel in abelowground coal mine",
    "While adresser was dressing the side of agallery after bl asting in the presence of asupervisor"
]

print("=== Testing Improved Text Correction ===")
for i, sample in enumerate(test_samples, 1):
    print(f"\nSample {i} Original:")
    print(sample)

    corrected = improved_text_correction(sample)
    print(f"Sample {i} Corrected:")
    print(corrected)
    print("-" * 70)

# Apply to your dataset with the improved function
print("\n=== Applying Improved Correction to Dataset ===")
print("Processing Description column...")
df['Description_fixed'] = df['Description'].apply(improved_text_correction)

print("Processing Suggestions column...")
df['Suggestions_fixed'] = df['Suggestions'].apply(improved_text_correction)

print("Improved text correction completed!")

# Show examples of the better corrections
print("\n=== Sample of Improved Corrections ===")
for i in range(3):
    print(f"\nExample {i+1}:")
    print("Original:")
    print(df['Description'].iloc[i][:150] + "...")
    print("\nCorrected:")
    print(df['Description_fixed'].iloc[i][:150] + "...")
    print("-" * 80)

# Fix the NLTK download issue
import nltk

# Download the missing punkt_tab resource
nltk.download('punkt_tab')

# Also download other required resources to be safe
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')  # Additional wordnet data

print("NLTK resources downloaded successfully!")

import pandas as pd
import numpy as np
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
import re
import string
from collections import Counter

class TopicModelingPreprocessor:
    def __init__(self):
        self.lemmatizer = WordNetLemmatizer()

        # Standard English stop words
        self.stop_words = set(stopwords.words('english'))

        # Add domain-specific stop words that won't be useful for topic modeling
        mining_stopwords = {
            'mine', 'mining', 'coal', 'accident', 'person', 'work', 'working',
            'required', 'regulation', 'provisions', 'mines', 'regulations',
            'employed', 'safety', 'one', 'two', 'three', 'four', 'five',
            'shall', 'must', 'should', 'would', 'could', 'may', 'might',
            'said', 'make', 'made', 'get', 'got', 'take', 'taken', 'give',
            'given', 'use', 'used', 'using', 'way', 'time', 'day', 'days',
            'year', 'years', 'place', 'area', 'site', 'location', 'cm', 'm',
            'mm', 'kg', 'ton', 'tonne', 'meter', 'metre', 'length', 'width',
            'height', 'depth', 'level', 'surface', 'ground'
        }

        self.stop_words.update(mining_stopwords)

    def get_wordnet_pos(self, word):
        """Map POS tag to first character lemmatize() accepts"""
        try:
            tag = nltk.pos_tag([word])[0][1][0].upper()
            tag_dict = {"J": wordnet.ADJ,
                        "N": wordnet.NOUN,
                        "V": wordnet.VERB,
                        "R": wordnet.ADV}
            return tag_dict.get(tag, wordnet.NOUN)
        except:
            return wordnet.NOUN

    def preprocess_for_topic_modeling(self, text):
        """
        Comprehensive preprocessing for topic modeling
        """
        if pd.isna(text) or text == '':
            return ''

        text = str(text).lower()

        # Remove special characters, numbers, and punctuation
        text = re.sub(r'[^a-zA-Z\s]', '', text)

        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()

        # Tokenize
        try:
            tokens = word_tokenize(text)
        except:
            # Fallback to simple split if word_tokenize fails
            tokens = text.split()

        # Filter and lemmatize tokens
        processed_tokens = []
        for token in tokens:
            # Keep only alphabetic tokens with length > 2
            if (token.isalpha() and
                len(token) > 2 and
                token not in self.stop_words):

                # Lemmatize with POS tagging
                try:
                    lemmatized = self.lemmatizer.lemmatize(token, self.get_wordnet_pos(token))
                    processed_tokens.append(lemmatized)
                except:
                    # Fallback to basic lemmatization
                    lemmatized = self.lemmatizer.lemmatize(token)
                    processed_tokens.append(lemmatized)

        return ' '.join(processed_tokens)

# Initialize the preprocessor
print("Initializing topic modeling preprocessor...")
preprocessor = TopicModelingPreprocessor()

# Create combined text field for topic modeling
print("Creating combined text field...")
# Use the corrected text columns from previous step
if 'Description_fixed' in df.columns and 'Suggestions_fixed' in df.columns:
    df['combined_text'] = df['Description_fixed'].fillna('') + ' ' + df['Suggestions_fixed'].fillna('')
else:
    # Fallback to original columns if fixed versions don't exist
    df['combined_text'] = df['Description'].fillna('') + ' ' + df['Suggestions'].fillna('')

print("Applying comprehensive preprocessing...")
df['processed_text'] = df['combined_text'].apply(preprocessor.preprocess_for_topic_modeling)

# Remove rows with empty processed text
initial_count = len(df)
df = df[df['processed_text'].str.strip() != '']
final_count = len(df)

print(f"Removed {initial_count - final_count} rows with empty processed text")
print(f"Final dataset size: {final_count} documents")

# Display preprocessing results
print("\n=== Preprocessing Results ===")
for i in range(min(3, len(df))):
    print(f"\nDocument {i+1}:")
    print("Original combined text:")
    print(df['combined_text'].iloc[i][:200] + "...")
    print("\nProcessed text:")
    print(df['processed_text'].iloc[i][:200] + "...")
    print("-" * 80)

# Basic statistics about processed text
all_processed_text = ' '.join(df['processed_text'])
all_words = all_processed_text.split()

print(f"\n=== Text Statistics ===")
print(f"Total processed words: {len(all_words):,}")
print(f"Unique words: {len(set(all_words)):,}")
print(f"Average words per document: {len(all_words)/len(df):.1f}")

# Most common words after preprocessing
word_freq = Counter(all_words).most_common(20)
print(f"\nTop 20 most frequent words after preprocessing:")
for word, freq in word_freq:
    print(f"{word:15} : {freq:4d}")

# Check document lengths
doc_lengths = df['processed_text'].str.split().str.len()
print(f"\nDocument length statistics:")
print(f"Min words: {doc_lengths.min()}")
print(f"Max words: {doc_lengths.max()}")
print(f"Mean words: {doc_lengths.mean():.1f}")
print(f"Median words: {doc_lengths.median():.1f}")

# Filter out very short documents (optional)
min_words = 5
long_enough = doc_lengths >= min_words
df_filtered = df[long_enough].copy()

print(f"\nFiltered out {len(df) - len(df_filtered)} documents with < {min_words} words")
print(f"Final dataset for topic modeling: {len(df_filtered)} documents")

print("\nStep 3 completed successfully!")

# Step 4: LDA Topic Modeling with 5 Topics
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# Set style for visualizations
plt.style.use('default')
sns.set_palette("husl")

print("=== Step 4: Topic Modeling with 5 Topics ===")

# 1. Create Document-Term Matrix using TF-IDF
print("\n1. Creating document-term matrix...")

vectorizer = TfidfVectorizer(
    max_features=1000,        # Limit to top 1000 features
    min_df=2,                 # Word must appear in at least 2 documents
    max_df=0.7,               # Word should not appear in more than 70% of documents
    ngram_range=(1, 2)        # Use both single words and bigrams
)

# Create the document-term matrix
doc_term_matrix = vectorizer.fit_transform(df_filtered['processed_text'])
feature_names = vectorizer.get_feature_names_out()

print(f"Document-term matrix shape: {doc_term_matrix.shape}")
print(f"Vocabulary size: {len(feature_names)}")

# 2. Train LDA Model with 5 Topics
print(f"\n2. Training LDA model with 5 topics...")

lda_model = LatentDirichletAllocation(
    n_components=5,           # Fixed to 5 topics
    random_state=42,
    max_iter=50,
    learning_method='batch',
    doc_topic_prior=0.1,      # Alpha: controls document-topic density
    topic_word_prior=0.01     # Beta: controls topic-word density
)

# Fit the model and get topic distributions
lda_output = lda_model.fit_transform(doc_term_matrix)

print(f"Model training completed!")
print(f"Model perplexity: {lda_model.perplexity(doc_term_matrix):.2f}")
print(f"Model log-likelihood: {lda_model.score(doc_term_matrix):.2f}")

# 3. Extract and Display Topics
print(f"\n3. Extracting and analyzing 5 topics...")

def display_topics_detailed(lda_model, feature_names, num_top_words=15):
    """Display top words for each topic with detailed analysis"""

    topics_data = []

    for topic_idx, topic in enumerate(lda_model.components_):
        # Get top words for this topic
        top_word_indices = topic.argsort()[-num_top_words:][::-1]
        top_words = [feature_names[i] for i in top_word_indices]
        top_weights = [topic[i] for i in top_word_indices]

        topics_data.append({
            'topic_id': topic_idx,
            'top_words': top_words,
            'weights': top_weights
        })

        print(f"\nüè∑Ô∏è  **Topic {topic_idx + 1}:**")
        print("   Top 5 words: " + " | ".join([f"{word}({weight:.3f})" for word, weight in zip(top_words[:5], top_weights[:5])]))
        print("   All top words: " + " | ".join(top_words))

    return topics_data

topics_info = display_topics_detailed(lda_model, feature_names, num_top_words=15)

# 4. Analyze Topic Distribution
print(f"\n4. Analyzing topic distribution across documents...")

# Get topic probabilities for each document
doc_topic_df = pd.DataFrame(lda_output, columns=[f'Topic_{i+1}' for i in range(5)])
doc_topic_df['Dominant_Topic'] = doc_topic_df.idxmax(axis=1)
doc_topic_df['Max_Probability'] = doc_topic_df.iloc[:, :-1].max(axis=1)

# Add original data for context
doc_topic_df['Year'] = df_filtered['Year'].values
doc_topic_df['Description'] = df_filtered['Description'].values

# Topic distribution
topic_counts = doc_topic_df['Dominant_Topic'].value_counts().sort_index()

print("\nüìä **Topic Distribution:**")
for topic, count in topic_counts.items():
    percentage = (count / len(doc_topic_df)) * 100
    print(f"{topic}: {count} documents ({percentage:.1f}%)")

# 5. Interpret Topics with Sample Documents
print(f"\n5. Sample documents for each topic...")

for i in range(5):
    topic_name = f'Topic_{i+1}'
    topic_docs = doc_topic_df[doc_topic_df['Dominant_Topic'] == topic_name]

    print(f"\nüîç **{topic_name} - Sample Documents:**")

    # Get top 2 documents for this topic (highest probability)
    top_docs = topic_docs.nlargest(2, 'Max_Probability')

    for idx, (_, doc) in enumerate(top_docs.iterrows()):
        print(f"   Sample {idx+1} (Prob: {doc['Max_Probability']:.3f}):")
        desc = doc['Description'][:200] + "..." if len(str(doc['Description'])) > 200 else str(doc['Description'])
        print(f"   {desc}")

# 6. Create Comprehensive Visualizations
print(f"\n6. Creating visualizations...")

fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# 6.1 Topic Distribution Pie Chart
axes[0, 0].pie(topic_counts.values, labels=topic_counts.index, autopct='%1.1f%%', startangle=90)
axes[0, 0].set_title('Distribution of Dominant Topics')

# 6.2 Topic-Word Heatmap
topic_word_matrix = []
word_labels = []

for i, topic_data in enumerate(topics_info):
    topic_words = topic_data['top_words'][:8]
    topic_weights = topic_data['weights'][:8]
    topic_word_matrix.append(topic_weights)
    if i == 0:
        word_labels = topic_words

topic_word_matrix = np.array(topic_word_matrix)

sns.heatmap(topic_word_matrix,
            xticklabels=word_labels,
            yticklabels=[f'Topic {i+1}' for i in range(5)],
            cmap='YlOrRd',
            annot=True,
            fmt='.3f',
            ax=axes[0, 1],
            cbar_kws={'label': 'Word Weight'})
axes[0, 1].set_title('Topic-Word Weight Matrix')
axes[0, 1].tick_params(axis='x', rotation=45)

# 6.3 Document Length vs Topic Confidence
doc_lengths = df_filtered['processed_text'].str.split().str.len()
scatter = axes[0, 2].scatter(doc_lengths, doc_topic_df['Max_Probability'],
                            c=doc_topic_df['Dominant_Topic'].astype('category').cat.codes,
                            cmap='tab10', alpha=0.6)
axes[0, 2].set_xlabel('Document Length (words)')
axes[0, 2].set_ylabel('Max Topic Probability')
axes[0, 2].set_title('Document Length vs Topic Confidence')

# 6.4 Topic Distribution by Year
topic_year_data = pd.concat([df_filtered[['Year']], doc_topic_df[['Dominant_Topic']]], axis=1)
topic_year_counts = topic_year_data.groupby(['Year', 'Dominant_Topic']).size().unstack(fill_value=0)

topic_year_counts.plot(kind='bar', stacked=True, ax=axes[1, 0])
axes[1, 0].set_title('Topic Distribution by Year')
axes[1, 0].set_xlabel('Year')
axes[1, 0].set_ylabel('Number of Documents')
axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')

# 6.5 Average Topic Probabilities
avg_topic_probs = doc_topic_df.iloc[:, :5].mean()
axes[1, 1].bar(range(1, 6), avg_topic_probs)
axes[1, 1].set_xlabel('Topic')
axes[1, 1].set_ylabel('Average Probability')
axes[1, 1].set_title('Average Topic Probabilities Across All Documents')
axes[1, 1].set_xticks(range(1, 6))
axes[1, 1].set_xticklabels([f'Topic {i}' for i in range(1, 6)])

# 6.6 Topic Coherence (Word Cloud Style - Top Words)
axes[1, 2].axis('off')
topic_text = ""
for i, topic_data in enumerate(topics_info):
    topic_text += f"Topic {i+1}: " + " ".join(topic_data['top_words'][:5]) + "\n"

axes[1, 2].text(0.1, 0.9, topic_text, transform=axes[1, 2].transAxes,
                fontsize=10, verticalalignment='top', fontfamily='monospace')
axes[1, 2].set_title('Top 5 Words per Topic')

plt.tight_layout()
plt.show()

# 7. Export Results
print(f"\n7. Exporting results...")

# Create a summary DataFrame
topic_summary = pd.DataFrame({
    'Topic': [f'Topic_{i+1}' for i in range(5)],
    'Document_Count': [topic_counts.get(f'Topic_{i+1}', 0) for i in range(5)],
    'Percentage': [topic_counts.get(f'Topic_{i+1}', 0) / len(doc_topic_df) * 100 for i in range(5)],
    'Top_5_Words': [' | '.join(topics_info[i]['top_words'][:5]) for i in range(5)],
    'Top_10_Words': [' | '.join(topics_info[i]['top_words'][:10]) for i in range(5)]
})

print("\nüìã **Topic Summary:**")
print(topic_summary.to_string(index=False))

# Save detailed results
doc_topic_df.to_excel('topic_modeling_results_5topics.xlsx', index=False)
topic_summary.to_excel('topic_summary_5topics.xlsx', index=False)

print(f"\n‚úÖ **5-Topic Modeling Complete!**")
print(f"üìÅ Results saved to:")
print(f"   - topic_modeling_results_5topics.xlsx (detailed document-topic assignments)")
print(f"   - topic_summary_5topics.xlsx (topic summary)")

